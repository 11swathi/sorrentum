{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c824b33d",
   "metadata": {},
   "source": [
    "Show Parquet / Pyarrow API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0113fbd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import numpy as np\n",
    "\n",
    "import helpers.dbg as dbg\n",
    "\n",
    "dbg.init_logger(verbosity=logging.INFO)\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ff89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create pandas random data, like:\n",
    "    \n",
    "                idx instr  val1  val2\n",
    "    2000-01-01    0     A    99    30\n",
    "    2000-01-02    0     A    54    46\n",
    "    2000-01-03    0     A    85    86\n",
    "    \"\"\"\n",
    "    num_rows = 100\n",
    "    instruments = \"A B C D E\".split()\n",
    "    cols = \"id stock val1 val2\".split()\n",
    "    df_idx = pd.date_range(pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-15\"), freq=\"1D\")\n",
    "    #print(df_idx)\n",
    "    random.seed(1000)\n",
    "\n",
    "    df = []\n",
    "    for idx, inst in enumerate(instruments):\n",
    "        df_tmp = pd.DataFrame({\"idx\": idx,\n",
    "                               \"instr\": inst,\n",
    "                               \"val1\": [random.randint(0, 100) for k in range(len(df_idx))],\n",
    "                               \"val2\": [random.randint(0, 100) for k in range(len(df_idx))],\n",
    "                              }, index=df_idx)\n",
    "        #print(df_tmp)\n",
    "        df.append(df_tmp)\n",
    "    df = pd.concat(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090edf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_str(df: pd.DataFrame) -> str:\n",
    "    txt = \"\"\n",
    "    txt += \"# df=\\n%s\" % df.head(3)\n",
    "    txt += \"\\n# df.shape=\\n%s\" % str(df.shape)\n",
    "    txt += \"\\n# df.dtypes=\\n%s\" % str(df.dtypes)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fd476",
   "metadata": {},
   "source": [
    "# Save and load all data in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "#print(df.head())\n",
    "print(df_to_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "print(\"table=\\n%s\" % table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77994547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save.\n",
    "file_name = \"df_in_one_file.pq\"\n",
    "pq.write_table(table, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load.\n",
    "df2 = pq.read_table(file_name)\n",
    "print(df2)\n",
    "\n",
    "df2 = df2.to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e963ab3",
   "metadata": {},
   "source": [
    "## Read a subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pq.read_table(file_name, columns=[\"idx\", \"val1\"])\n",
    "print(df2)\n",
    "\n",
    "df2 = df2.to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728366b3",
   "metadata": {},
   "source": [
    "## Partitioned dataset\n",
    "\n",
    "from https://arrow.apache.org/docs/python/dataset.html#reading-partitioned-data\n",
    "\n",
    "- A dataset can exploit a nested structure, where the sub-dir names hold information about which subset of the data is stored in that dir\n",
    "- E.g., \"Hive\" patitioning scheme \"key=vale\" dir names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b083370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "print(df_to_str(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \".\"\n",
    "dir_name =  os.path.join(base, \"parquet_dataset_partitioned\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "pq.write_to_dataset(table,\n",
    "                    dir_name,\n",
    "                    partition_cols=['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls parquet_dataset_partitioned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name,\n",
    "                     format=\"parquet\",\n",
    "                     partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64394b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read everything.\n",
    "df2 = dataset.to_table().to_pandas()\n",
    "\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load part of the data.\n",
    "\n",
    "df2 = dataset.to_table(filter=ds.field(\"idx\") == 1).to_pandas()\n",
    "print(df_to_str(df2))\n",
    "\n",
    "df2 = dataset.to_table(filter=ds.field(\"idx\") < 3).to_pandas()\n",
    "print(df_to_str(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df.index.year\n",
    "df[\"month\"] = df.index.month\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \".\"\n",
    "dir_name =  os.path.join(base, \"parquet_dataset_partitioned3\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "pq.write_to_dataset(table,\n",
    "                    dir_name,\n",
    "                    partition_cols=['idx', \"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $dir_name/idx=0/year=2000/month=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name,\n",
    "                     format=\"parquet\",\n",
    "                     partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4111d",
   "metadata": {},
   "source": [
    "## Partition manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b33d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow.dataset import DirectoryPartitioning\n",
    "\n",
    "partitioning = DirectoryPartitioning(pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int8())]))\n",
    "print(partitioning.parse(\"/2009/11/3\"))\n",
    "\n",
    "#partitioning.discover()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b82e0",
   "metadata": {},
   "source": [
    "## Read subset of columns for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21148afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data back.\n",
    "dataset = ds.dataset(dir_name,\n",
    "                     format=\"parquet\",\n",
    "                     partitioning=\"hive\")\n",
    "\n",
    "print(\"\\n\".join(dataset.files))\n",
    "\n",
    "dataset.to_table(filter=ds.field('idx') == 2).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa16cb5",
   "metadata": {},
   "source": [
    "## Read everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044dec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only one column.\n",
    "\n",
    "df2 = pq.read_table(\"example.pq\", columns=[\"idx\", \"val1\"])\n",
    "print(df2)\n",
    "\n",
    "df2 = df2.to_pandas()\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ddf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could scan manually and create the dirs manually if we don't want to add\n",
    "# add a new dir.\n",
    "base = \".\"\n",
    "dir_name =  os.path.join(base, \"parquet_dataset_partitioned2\")\n",
    "os.system(\"rm -rf %s\" % dir_name)\n",
    "\n",
    "grouped = df.groupby(lambda x: x.day)\n",
    "for day, df_tmp in grouped:\n",
    "    print(day, df_tmp)\n",
    "    grouped2 = df_tmp.groupby(\"idx\")\n",
    "    for id_, df_tmp2 in grouped2:\n",
    "        print(day, id_, df_tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148be7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to merge PQ files\n",
    "\n",
    "# We can filter by year, month, stock and then all save in the same dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc382c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef75c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d1189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.11.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
